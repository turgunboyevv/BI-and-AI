{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3\n",
    "You are tasked with scraping laptop data from the \"Laptops\" section of the Demoblaze website and storing the extracted information in JSON format.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Navigate to the Website:\n",
    "\n",
    "Visit the Demoblaze homepage.\n",
    "Click on the Laptops section to view the list of available laptops.\n",
    "Navigate to the Next Page:\n",
    "\n",
    "After reaching the Laptops section, locate and click the Next button to navigate to the next page of laptop listings.\n",
    "Data to Scrape: For each laptop on the page, scrape the following details:\n",
    "\n",
    "Laptop Name\n",
    "Price\n",
    "Description\n",
    "Data Storage:\n",
    "\n",
    "Save the extracted information in a structured JSON format with fields like:\n",
    "[\n",
    "  {\n",
    "    \"name\": \"Laptop Name\",\n",
    "    \"price\": \"Laptop Price\",\n",
    "    \"description\": \"Laptop Description\"\n",
    "  },\n",
    "  ...\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://www.demoblaze.com\"\n",
    "LAPTOPS_URL = f\"{BASE_URL}/index.html#\"\n",
    "\n",
    "# Function to scrape laptop data from a single page\n",
    "def scrape_laptops_from_page(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    laptops = []\n",
    "    items = soup.find_all(\"div\", class_=\"col-lg-4 col-md-6 mb-4\")\n",
    "\n",
    "    for item in items:\n",
    "        name = item.find(\"h4\", class_=\"card-title\").text.strip()\n",
    "        price = item.find(\"h5\").text.strip()\n",
    "        description = item.find(\"p\", class_=\"card-text\").text.strip()\n",
    "\n",
    "        laptops.append({\n",
    "            \"name\": name,\n",
    "            \"price\": price,\n",
    "            \"description\": description\n",
    "        })\n",
    "\n",
    "    return laptops\n",
    "\n",
    "# Function to navigate through all pages and scrape laptop data\n",
    "def scrape_all_laptops():\n",
    "    laptops = []\n",
    "    current_page_url = LAPTOPS_URL\n",
    "\n",
    "    while True:\n",
    "        laptops.extend(scrape_laptops_from_page(current_page_url))\n",
    "\n",
    "        # Check for the Next button\n",
    "        response = requests.get(current_page_url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        next_button = soup.find(\"button\", text=\"Next\")\n",
    "\n",
    "        if next_button:\n",
    "            current_page_url = f\"{BASE_URL}/{next_button['onclick'].split(\"'\")[1]}\"\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return laptops\n",
    "\n",
    "# Main execution flow\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape all laptops\n",
    "    all_laptops = scrape_all_laptops()\n",
    "\n",
    "    # Save the data to a JSON file\n",
    "    with open(\"laptops.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(all_laptops, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"Laptop data successfully scraped and saved to laptops.json.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
