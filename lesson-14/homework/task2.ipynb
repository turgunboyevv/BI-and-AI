{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2\n",
    "Scrape job listings from the website https://realpython.github.io/fake-jobs and store the data into an SQLite database.\n",
    "\n",
    "Scraping Requirements:\n",
    "\n",
    "Extract the following details for each job listing:\n",
    "Job Title\n",
    "Company Name\n",
    "Location\n",
    "Job Description\n",
    "Application Link\n",
    "Data Storage:\n",
    "\n",
    "Store the scraped data into an SQLite database in a table named jobs.\n",
    "Incremental Load:\n",
    "\n",
    "Ensure that your script performs incremental loading:\n",
    "Scrape the webpage and add only new job listings to the database.\n",
    "Avoid duplicating entries. Use Job Title, Company Name, and Location as unique identifiers for comparison.\n",
    "Update Tracking:\n",
    "\n",
    "Add functionality to detect if an existing job listing has been updated (e.g., description or application link changes) and update the database record accordingly.\n",
    "Filtering and Exporting:\n",
    "\n",
    "Allow filtering job listings by location or company name.\n",
    "Write a function to export filtered results into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "# Constants\n",
    "URL = \"https://realpython.github.io/fake-jobs\"\n",
    "DB_NAME = \"jobs.db\"\n",
    "\n",
    "# Function to create the database and table\n",
    "def initialize_database():\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS jobs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            job_title TEXT,\n",
    "            company_name TEXT,\n",
    "            location TEXT,\n",
    "            description TEXT,\n",
    "            application_link TEXT,\n",
    "            UNIQUE(job_title, company_name, location)\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Function to scrape job listings\n",
    "def scrape_jobs():\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    jobs = []\n",
    "    for job_element in soup.find_all(\"div\", class_=\"card-content\"):\n",
    "        job_title = job_element.find(\"h2\", class_=\"title is-5\").text.strip()\n",
    "        company_name = job_element.find(\"h3\", class_=\"subtitle is-6 company\").text.strip()\n",
    "        location = job_element.find(\"p\", class_=\"location\").text.strip()\n",
    "        description = job_element.find(\"div\", class_=\"content\").text.strip()\n",
    "        application_link = job_element.find(\"a\", text=\"Apply\")['href']\n",
    "\n",
    "        jobs.append({\n",
    "            \"job_title\": job_title,\n",
    "            \"company_name\": company_name,\n",
    "            \"location\": location,\n",
    "            \"description\": description,\n",
    "            \"application_link\": application_link,\n",
    "        })\n",
    "    return jobs\n",
    "\n",
    "# Function to store jobs into the database\n",
    "def store_jobs(jobs):\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for job in jobs:\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO jobs (job_title, company_name, location, description, application_link)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "        ''', (job['job_title'], job['company_name'], job['location'], job['description'], job['application_link']))\n",
    "\n",
    "        # Check if an existing job has been updated\n",
    "        cursor.execute('''\n",
    "            UPDATE jobs\n",
    "            SET description = ?, application_link = ?\n",
    "            WHERE job_title = ? AND company_name = ? AND location = ?\n",
    "              AND (description != ? OR application_link != ?)\n",
    "        ''', (job['description'], job['application_link'], job['job_title'], job['company_name'], job['location'], job['description'], job['application_link']))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Function to filter jobs by location or company name\n",
    "def filter_jobs(location=None, company_name=None):\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    query = \"SELECT * FROM jobs WHERE 1=1\"\n",
    "    params = []\n",
    "\n",
    "    if location:\n",
    "        query += \" AND location = ?\"\n",
    "        params.append(location)\n",
    "    if company_name:\n",
    "        query += \" AND company_name = ?\"\n",
    "        params.append(company_name)\n",
    "\n",
    "    cursor.execute(query, params)\n",
    "    results = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return results\n",
    "\n",
    "# Function to export filtered jobs to a CSV file\n",
    "def export_to_csv(jobs, filename):\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Job Title\", \"Company Name\", \"Location\", \"Description\", \"Application Link\"])\n",
    "        for job in jobs:\n",
    "            writer.writerow(job)\n",
    "\n",
    "# Main execution flow\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize database\n",
    "    initialize_database()\n",
    "\n",
    "    # Scrape jobs from the webpage\n",
    "    jobs = scrape_jobs()\n",
    "\n",
    "    # Store jobs into the database\n",
    "    store_jobs(jobs)\n",
    "\n",
    "    # Example usage: Filter jobs by location or company name\n",
    "    location_filter = \"Remote\"\n",
    "    company_filter = \"Company XYZ\"\n",
    "    filtered_jobs = filter_jobs(location=location_filter, company_name=company_filter)\n",
    "\n",
    "    # Export filtered jobs to a CSV file\n",
    "    export_to_csv(filtered_jobs, \"filtered_jobs.csv\")\n",
    "\n",
    "    print(\"Job scraping and processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
